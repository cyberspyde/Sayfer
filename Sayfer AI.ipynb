{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc12493",
   "metadata": {},
   "source": [
    "# Sayfer AI dasturiga xush kelibsiz! \n",
    "\n",
    "- Jarvis dasturiga qiziqadigan odamlar anchagina, yoshlikdan shu sohada biror yangilik yoki dastur chiqmasin, barchasini sinab ko'rish, foydalanishga xohishim baland bo'lgani sabab, O'zbek tilida ham shunday imkoniyatni, shunday dasturiy yengillikni yaratishga maqsad qilganman. \n",
    "\n",
    "- Albatta ishlatilgan kodlarni barchasi maniki dimeman, ba'zilari official docs dan olingan, ba'zilari stackoverflow dan olingan, va ba'zilarini esa pythonni o'rganib, o'zim 0 dan tuzib chiqganman, maqsad to'liq O'zbek tilidan ishlaydigan virtual yordamchi yaratish ekan, ortiqcha so'zlarsiz proyektdan foyda ko'rganlar imkon bo'lsa duoda eslab qo'yilar:). -- -- Hammaga rahmat!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f642110c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 0) Initialization (kerakli librarylarni yuklash)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "90d2d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import torch, string, random, json, subprocess, wikipedia, os, requests, threading, uuid, nltk, openai\n",
    "from subprocess import call, Popen\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from playsound import playsound\n",
    "from tkinter import *\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "from transformers import RobertaTokenizer, RobertaForQuestionAnswering\n",
    "\n",
    "mydate = datetime.today().strftime(\"%y-%m-%d\")\n",
    "\n",
    "key_file = open('../key')\n",
    "\n",
    "key_data = json.load(key_file)\n",
    "resource_key = key_data['resource_key']\n",
    "region = key_data['region']\n",
    "endpoint = key_data['endpoint']\n",
    "path = key_data['path']\n",
    "KEY_FILE = key_data['key_file']\n",
    "LOCATION = key_data['location']\n",
    "openai.api_key = key_data['openapi_key']\n",
    "\n",
    "key_file.close()\n",
    "\n",
    "\n",
    "start_sequence = \"\\nAI:\"\n",
    "restart_sequence = \"\\nHuman: \"\n",
    "\n",
    "session_prompt = \"The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\\n\\nHuman: Hello, who are you?\\nAI: I am an AI created by OpenAI. How can I help you today?\\nHuman: \"\n",
    "\n",
    "speech_key, service_region = KEY_FILE, LOCATION\n",
    "speech_config_stt = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "speech_config_stt.speech_recognition_language=\"uz-UZ\"\n",
    "#audio_config_stt = speechsdk.audio.AudioConfig(device_name=\"{0.0.1.00000000}.{c91342cf-e781-4464-93ef-0a8e17804c64}\") #headset\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config_stt)\n",
    "\n",
    "audio_config_tts = speechsdk.audio.AudioConfig(device_name='c91342cf-e781-4464-93ef-0a8e17804c64') #computer\n",
    "speech_config_tts = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "speech_config_tts.speech_synthesis_voice_name = \"uz-UZ-MadinaNeural\"\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config_tts)\n",
    "\n",
    "conf_file = open('assets/settings.conf')\n",
    "conf_data = json.load(conf_file)\n",
    "\n",
    "voice_activation = conf_data['voice_activation']\n",
    "robot_name = conf_data['robot_name'].lower()\n",
    "gpt3 = conf_data['gpt3']\n",
    "robertaqna_settings = conf_data['robertaqna']\n",
    "conf_file.close()\n",
    "\n",
    "    \n",
    "wikipedia.set_lang(\"uz\")\n",
    "\n",
    "#Initialization of robertaqna model parameters\n",
    "\n",
    "categories = ['sports', 'science', 'technology', 'politics', 'business', 'society']\n",
    "\n",
    "sport = open('assets/sports-knowledge.json')\n",
    "sport_knowledge = json.load(sport)\n",
    "sport_news = sport_knowledge['news']\n",
    "sport.close()\n",
    "\n",
    "technology = open('assets/technology-knowledge.json')\n",
    "technology_knowledge = json.load(technology)\n",
    "technology_news = technology_knowledge['news']\n",
    "technology.close()\n",
    "\n",
    "politics = open('assets/politics-knowledge.json')\n",
    "politics_knowledge = json.load(politics)\n",
    "politics_news = politics_knowledge['news']\n",
    "politics.close()\n",
    "\n",
    "society = open('assets/society-knowledge.json')\n",
    "society_knowledge = json.load(society)\n",
    "society_news = society_knowledge['news']\n",
    "society.close()\n",
    "\n",
    "culture = open('assets/culture-knowledge.json')\n",
    "culture_knowledge = json.load(culture)\n",
    "culture_news = culture_knowledge['news']\n",
    "culture.close()\n",
    "\n",
    "business = open('assets/business-knowledge.json')\n",
    "business_knowledge = json.load(business)\n",
    "business_news = business_knowledge['news']\n",
    "business.close()\n",
    "\n",
    "factual = open('assets/factual-knowledge.json')\n",
    "factual_knowledge = json.load(factual)\n",
    "factual_knowledge_all = factual_knowledge['all']\n",
    "factual.close()\n",
    "\n",
    "analytical = open('assets/analytical-knowledge.json')\n",
    "analytical_knowledge = json.load(analytical)\n",
    "analytical_knowledge_all = analytical_knowledge['all']\n",
    "analytical.close()\n",
    "\n",
    "subjective = open('assets/subjective-knowledge.json')\n",
    "subjective_knowledge = json.load(subjective)\n",
    "subjective_knowledge_all = subjective_knowledge['all']\n",
    "subjective.close()\n",
    "\n",
    "objective = open('assets/objective-knowledge.json')\n",
    "objective_knowledge = json.load(objective)\n",
    "objective_knowledge_all = objective_knowledge['all']\n",
    "objective.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e58c8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 1) Birinchi NLTK library dan foydalanib ishlatiladigan funksyilarni ishga tushiramiz.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "465061e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(sentence):\n",
    "\treturn nltk.word_tokenize(sentence)\n",
    "\n",
    "def stem(word):\n",
    "\treturn stemmer.stem(word.lower())\n",
    "\n",
    "\n",
    "def bag_of_words(tokenized_sentence, all_words):\n",
    "\ttokenized_sentence = [stem(w) for w in tokenized_sentence]\n",
    "\n",
    "\tbag = np.zeros(len(all_words), dtype=np.float32)\n",
    "\tfor idx, w in enumerate(all_words):\n",
    "\t\tif w in tokenized_sentence:\n",
    "\t\t\tbag[idx] = 1.0\n",
    "\n",
    "\treturn bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad0ddd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 2) NLTK Utils to'liq ishlashi uchun punkt modeli kerak bo'ladi, endi uni yuklab olishimiz kerak.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6f48601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Peter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83803aad",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 3) Uchinchi qilgan ishimiz, pytorch orqali modelni yaratish, va uni yuklash\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "702b3432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\tdef __init__(self, input_size, hidden_size, num_classes):\n",
    "\t\tsuper(NeuralNet, self).__init__()\n",
    "\t\tself.l1 = nn.Linear(input_size, hidden_size)\n",
    "\t\tself.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "\t\tself.l3 = nn.Linear(hidden_size, num_classes)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = self.l1(x)\n",
    "\t\tout = self.relu(out)\n",
    "\t\tout = self.l2(out)\n",
    "\t\tout = self.relu(out)\n",
    "\t\tout = self.l3(out)\n",
    "\n",
    "\t\t#no activation no softmax\n",
    "\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0e96a8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 4) Endi novbat, modelni intents.json fayl imiz orqali shug'ullantirish (training)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69789d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.7232\n",
      "Epoch [200/1000], Loss: 0.0603\n",
      "Epoch [300/1000], Loss: 0.0080\n",
      "Epoch [400/1000], Loss: 0.0044\n",
      "Epoch [500/1000], Loss: 0.0023\n",
      "Epoch [600/1000], Loss: 0.0003\n",
      "Epoch [700/1000], Loss: 0.0012\n",
      "Epoch [800/1000], Loss: 0.0005\n",
      "Epoch [900/1000], Loss: 0.0005\n",
      "Epoch [1000/1000], Loss: 0.0000\n",
      "final loss: 0.0000\n",
      "training complete. file saved to model/data-20-07-2022-15-54-36.pth\n"
     ]
    }
   ],
   "source": [
    "with open('assets/intents.json') as f:\n",
    "\tintents = json.load(f)\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "\n",
    "for intent in intents['intents']:\n",
    "\ttag = intent['tag']\n",
    "\ttags.append(tag)\n",
    "\tfor pattern in intent['patterns']:\n",
    "\t\tw = tokenize(pattern)\n",
    "\t\tall_words.extend(w)\n",
    "\t\txy.append((w, tag))\n",
    "\n",
    "ignore_words = ['?', '!', ',', '.']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for (pattern_sentence, tag) in xy:\n",
    "\tbag = bag_of_words(pattern_sentence, all_words)\n",
    "\tx_train.append(bag)\n",
    "\n",
    "\tlabel = tags.index(tag)\n",
    "\ty_train.append(label)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "\tdef __init__(self):\n",
    "\t\tself.n_samples = len(x_train)\n",
    "\t\tself.x_data = x_train\n",
    "\t\tself.y_data = y_train\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn self.x_data[index], self.y_data[index]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.n_samples\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "input_size = len(x_train[0])\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "\n",
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(words)\n",
    "        # if y would be one-hot, we must apply\n",
    "        # labels = torch.max(labels, 1)[1]\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "print(f'final loss: {loss.item():.4f}')\n",
    "\n",
    "data = {\n",
    "\"model_state\": model.state_dict(),\n",
    "\"input_size\": input_size,\n",
    "\"hidden_size\": hidden_size,\n",
    "\"output_size\": output_size,\n",
    "\"all_words\": all_words,\n",
    "\"tags\": tags\n",
    "}\n",
    "\n",
    "date = datetime.now().strftime('%d-%m-%Y-%H-%M-%S')\n",
    "\n",
    "FILE = f\"model/data-{date}.pth\"\n",
    "\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802d400",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 5) Modeldan javob olish\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed36ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open('assets/intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "FILE = \"model/data.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data[\"all_words\"]\n",
    "tags = data[\"tags\"]\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "bot_name = robot_name\n",
    "\n",
    "def get_response(msg):\n",
    "\n",
    "    sentence = tokenize(msg)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "\n",
    "    if prob.item() > 0.75:\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                answer = random.choice(intent['responses'])\n",
    "                return answer\n",
    "    \n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9f493",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 6) Savol va Javoblarni Ingliz tilidan Uzbek tiliga va Uzbek tilidan Ingliz tiliga tarjima qilish funksiyalari\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6666df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_to_uzbek(text):\n",
    "    params = '&from=en&to=uz'\n",
    "    constructed_url = endpoint + path + params\n",
    "\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': resource_key,\n",
    "        'Ocp-Apim-Subscription-Region': region,\n",
    "        'Content-type': 'application/json',\n",
    "        'X-ClientTraceId': str(uuid.uuid4())\n",
    "    }\n",
    "\n",
    "    body = [{\n",
    "        'text' : str(text)\n",
    "    }]\n",
    "    request = requests.post(constructed_url, headers=headers, json=body)\n",
    "    response = request.json()\n",
    "    ans = json.loads(json.dumps(response, sort_keys=True, indent=4, separators=(',', ': ')))\n",
    "\n",
    "\n",
    "    return ans[0]['translations'][0]['text']\n",
    "\n",
    "def uzbek_to_english(text):\n",
    "    params = '&from=uz&to=en'\n",
    "    constructed_url = endpoint + path + params\n",
    "\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': resource_key,\n",
    "        'Ocp-Apim-Subscription-Region': region,\n",
    "        'Content-type': 'application/json',\n",
    "        'X-ClientTraceId': str(uuid.uuid4())\n",
    "    }\n",
    "\n",
    "    body = [{\n",
    "        'text' : str(text)\n",
    "    }]\n",
    "    request = requests.post(constructed_url, headers=headers, json=body)\n",
    "    response = request.json()\n",
    "    ans = json.loads(json.dumps(response, sort_keys=True, indent=4, separators=(',', ': ')))\n",
    "\n",
    "\n",
    "    return ans[0]['translations'][0]['text'] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffa1a0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 7) Ortiqcha belgilarni textdan olib tashlash\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16c61f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_characters(text):\n",
    "    a = text\n",
    "    b = \"!@#$()-_=+%&.,\\\" \"\n",
    "    \n",
    "    for char in b:\n",
    "        a = a.replace(char, \" \")    \n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c022e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 8) Kunlar va yillar qatnashgan tekstlarni ovozli eshittirishga moslashtirish\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "06805a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_first = {\n",
    "    1 : \"O'n\",\n",
    "    2 : \"Yigirma\",\n",
    "    3 : \"O'ttiz\"\n",
    "}\n",
    "\n",
    "day_second = {\n",
    "    1 : \"Birinchi\",\n",
    "    2 : \"Ikkinchi\",\n",
    "    3 : \"Uchinchi\",\n",
    "    4 : \"To'rtinchi\",\n",
    "    5 : \"Beshinchi\",\n",
    "    6 : \"Oltinchi\",\n",
    "    7 : \"Yettinchi\",\n",
    "    8 : \"Sakkizinchi\",\n",
    "    9 : \"To'qqizinchi\",\n",
    "    0 : \"inchi\"\n",
    "}\n",
    "\n",
    "\n",
    "def day_filter(answer):\n",
    "    answer = drop_characters(answer)\n",
    "\n",
    "    digits = [int(s) for s in answer.split() if s.isdigit() and int(s) <= 31 and int(s) >= 10]\n",
    "    digits2 = [int(s) for s in answer.split() if s.isdigit() and int(s) <= 9]\n",
    "\n",
    "    digits_in_text = []\n",
    "    digits_in_text2 = []\n",
    "\n",
    "    for digit in digits:\n",
    "        sp = [int(a) for a in str(digit)]\n",
    "        digits_in_text.append(sp)\n",
    "\n",
    "    for digit in digits2:\n",
    "        sp = [int(a) for a in str(digit)]\n",
    "        digits_in_text2.append(sp)\n",
    "\n",
    "    day_first_values = []\n",
    "    day_second_values = []\n",
    "\n",
    "    day_first_values2 = []\n",
    "\n",
    "    for p in digits_in_text:\n",
    "        day_first_values.append(str(p[0]).replace(str(p[0]), day_first[p[0]]))\n",
    "        day_second_values.append(str(p[1]).replace(str(p[1]), day_second[p[1]]))\n",
    "\n",
    "\n",
    "    for p in digits_in_text2:\n",
    "        day_first_values2.append(str(p[0]).replace(str(p[0]), day_second[p[0]]))\n",
    "\n",
    "    joint_days = {}\n",
    "    joint_days2 = {}\n",
    "\n",
    "    for t in range(0, len(digits)):\n",
    "        days_in_text = day_first_values[t] + \" \" + day_second_values[t]\n",
    "        joint_days[digits[t]] = days_in_text \n",
    "\n",
    "    for t in range(0, len(digits2)):\n",
    "        days_in_text2 = day_first_values2[t]\n",
    "        joint_days2[digits2[t]] = days_in_text2 \n",
    "\n",
    "    for _ in range(0, len(digits)):\n",
    "        for t in answer.split():\n",
    "            if(t.isdigit() and int(t) <= 31 and int(t) >= 10):\n",
    "                answer = answer.replace(t, joint_days[int(t)])\n",
    "\n",
    "    for _ in range(0, len(digits2)):\n",
    "        for t in answer.split():\n",
    "            if(t.isdigit() and int(t) <= 9):\n",
    "                answer = answer.replace(t, joint_days2[int(t)])\n",
    "\n",
    "    return answer\n",
    "\n",
    "def year_filter(answer):\n",
    "    year_first = {\n",
    "        1 : \"bir ming\",\n",
    "        2 : \"ikki ming\",\n",
    "        3 : \"uch ming\",\n",
    "        4 : \"to'rt ming\",\n",
    "        5 : \"besh ming\",\n",
    "        6 : \"olti ming\",\n",
    "        7 : \"yetti ming\",\n",
    "        8 : \"sakkiz ming\",\n",
    "        9 : \"to'qqiz ming\"\n",
    "    }\n",
    "\n",
    "    year_second = {\n",
    "        1 : \"bir yuz\",\n",
    "        2 : \"ikkiyuz\",\n",
    "        3 : \"uchyuz\",\n",
    "        4 : \"to'rtyuz\",\n",
    "        5 : \"beshyuz\",\n",
    "        6 : \"oltiyuz\",\n",
    "        7 : \"yettiyuz\",\n",
    "        8 : \"sakkizyuz\",\n",
    "        9 : \"to'qqizyuz\",\n",
    "        0 : \"\"\n",
    "    }\n",
    "\n",
    "    year_third = {\n",
    "        1 : \"o'n\",\n",
    "        2 : \"yigirma\",\n",
    "        3 : \"o'ttiz\",\n",
    "        4 : \"qirq\",\n",
    "        5 : \"ellik\",\n",
    "        6 : \"oltmish\",\n",
    "        7 : \"yetmish\",\n",
    "        8 : \"sakson\",\n",
    "        9 : \"to'qson\",\n",
    "        0 : \"\"\n",
    "\n",
    "    }\n",
    "\n",
    "    year_fourth = {\n",
    "        1 : \"birinchi\",\n",
    "        2 : \"ikkinchi\",\n",
    "        3 : \"uchinchi\",\n",
    "        4 : \"to'rtinchi\",\n",
    "        5 : \"beshinchi\",\n",
    "        6 : \"oltinchi\",\n",
    "        7 : \"yettinchi\",\n",
    "        8 : \"sakkizinchi\",\n",
    "        9 : \"to'qqizzinchi\",\n",
    "        0 : \"inchi\"\n",
    "    }\n",
    "\n",
    "    answer = drop_characters(answer)\n",
    "\n",
    "    digits = [int(s) for s in answer.split() if s.isdigit() and int(s) >= 1000]\n",
    "    \n",
    "    digits_in_text = []\n",
    "    for k in digits:\n",
    "        seperate_year_digits = [int(b) for b in str(k)]\n",
    "        digits_in_text.append(seperate_year_digits)\n",
    "\n",
    "    year_first_values = []\n",
    "    year_second_values = []\n",
    "    year_third_values = []\n",
    "    year_fourth_values = []\n",
    "\n",
    "\n",
    "    for p in digits_in_text:\n",
    "        year_first_values.append(str(p[0]).replace(str(p[0]), year_first[p[0]]))\n",
    "        year_second_values.append(str(p[1]).replace(str(p[1]), year_second[p[1]]))\n",
    "        year_third_values.append(str(p[2]).replace(str(p[2]), year_third[p[2]]))\n",
    "        year_fourth_values.append(str(p[3]).replace(str(p[3]), year_fourth[p[3]]))\n",
    "\n",
    "    joint_years = {}\n",
    "    for t in range(0, len(digits)):\n",
    "        years_in_text = year_first_values[t] + \" \" + year_second_values[t] + \" \" + year_third_values[t] + \" \" + year_fourth_values[t]\n",
    "        joint_years[digits[t]] = years_in_text \n",
    "\n",
    "\n",
    "    for _ in range(0, len(digits)):\n",
    "        for t in answer.split():\n",
    "            if(t.isdigit() and int(t) >= 1000):\n",
    "                answer = answer.replace(t, joint_years[int(t)])\n",
    "\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd8cb4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 9) Ayni vaqtni aniqlash funksiyasi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "238768b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_time():\n",
    "    answer = datetime.now().strftime(\"%H-%M\").split(\"-\",1)\n",
    "    hour = int(answer[0])\n",
    "    minute = int(answer[1])\n",
    "\n",
    "    if (minute > 9):\n",
    "        minute_digits = [int(a) for a in str(minute)]\n",
    "        minute_last = minute_digits[1]\n",
    "        minute_first = minute_digits[0]\n",
    "\n",
    "    minute_say = \"\"\n",
    "\n",
    "    if minute in range(10, 20):\n",
    "        minute_say = \"o'n\"\n",
    "    elif minute in range(20, 30):\n",
    "        minute_say = \"yigirma\"\n",
    "    elif minute in range(30, 40):\n",
    "        minute_say = \"o'ttiz\"\n",
    "    elif minute in range(40, 50):\n",
    "        minute_say = \"qirq\"\n",
    "    elif minute in range(50, 60):\n",
    "        minute_say = \"ellik\"\n",
    "\n",
    "\n",
    "\n",
    "    if (hour > 9):\n",
    "        hour_digits = [int(a) for a in str(hour)]\n",
    "        hour_last = hour_digits[1]\n",
    "        hour_first = hour_digits[0]\n",
    "\n",
    "    hour_say = \"\"\n",
    "\n",
    "    if hour in range(10, 20):\n",
    "        hour_say = \"o'n\"\n",
    "    elif hour in range(20, 24):\n",
    "        hour_say = \"yigirma\"\n",
    "\n",
    "\n",
    "\n",
    "    if(minute <= 9 and hour <= 9):\n",
    "        currentTime = f\"Soat {hour} dan, {minute} daqiqa o'tdi.\"\n",
    "    elif(minute >= 10 and hour <= 9):\n",
    "        currentTime = f\"Soat {hour} dan, {minute_say} {minute_last} daqiqa o'tdi.\"\n",
    "    elif(minute <= 9 and hour >= 10):\n",
    "        currentTime = f\"Soat {hour_say} {hour_last} dan, {minute} daqiqa o'tdi.\"\n",
    "    elif(minute >= 10 and hour >= 10 and hour_last != 0):\n",
    "        currentTime = f\"Soat {hour_say} {hour_last} dan, {minute_say} {minute_last} daqiqa o'tdi.\"\n",
    "    elif(minute >= 10 and hour >= 10 and hour_last == 0):\n",
    "        currentTime = f\"Soat {hour_say} dan, {minute} daqiqa o'tdi.\"\n",
    "    elif(minute >= 10 and hour >= 10 and hour_last == 0 and minute_last == 0):\n",
    "        currentTime = f\"Soat {hour_say} dan, {minute} daqiqa o'tdi.\"\n",
    "        \n",
    "    return currentTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f9f00e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_word(text, word):\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        if w == word:\n",
    "            words.remove(w)\n",
    "    answer = ' '.join(words)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de377056",
   "metadata": {},
   "source": [
    "## 10) Tekst klassifikatsiyasi modelini yuklash (text classification model from huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a505026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", cache_dir=\".cache/huggingface/transformers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e90931ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(query):\n",
    "    labels = [\"artifacts\", \"animals\", \"food\", \"birds\", \"fast food\", \"book\", \"education\", \"news\", \"failure\", \"emotion\", \"sad\",\n",
    "             \"happy\", \"time\", \"family\", \"math\", \"computer\", \"general\", \"language\", \"drinks\", \"alcohol\", \"religion\", \"Islam\",\n",
    "             \"cars\"]\n",
    "    hypothesis_template = \"This text is about {} .\"\n",
    "    prediction = text_classifier(query, labels, hypothesis_template=hypothesis_template, multi_label=True)\n",
    "\n",
    "    labels = prediction['labels'][:3]\n",
    "    scores = prediction['scores'][:3]\n",
    "    labels_selected = []\n",
    "    scores_selected = []\n",
    "    final_answer = []\n",
    "    #Qaytarilgan javoblardan 3 ta 75 dan balandlarini tanlab olish\n",
    "    for score in scores:\n",
    "        if score >= 0.75:\n",
    "            scores_selected.append(score)\n",
    "# Eng yaxshi natijani aniqlash\n",
    "    if len(scores_selected) == 1:\n",
    "        labels_selected.append(labels[0])\n",
    "    elif len(scores_selected) == 2:\n",
    "        labels_selected.append(labels[0])\n",
    "        labels_selected.append(labels[1])\n",
    "    elif len(scores_selected) == 3:\n",
    "        labels_selected.append(labels[0])\n",
    "        labels_selected.append(labels[1])\n",
    "        labels_selected.append(labels[2])\n",
    "    \n",
    "    if len(scores_selected) != 0:\n",
    "        final_answer.append(scores_selected[0])\n",
    "        final_answer.append(labels_selected[0])\n",
    "    else:\n",
    "        final_answer = \"No response\"\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b6425",
   "metadata": {},
   "source": [
    "### 11) Savollarga javob berish va o'rganish modelini yuklash ( huggingface, roberta-base-squad2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bba88f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_qna_tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\", cache_dir=\".cache/huggingface/transformers/\")\n",
    "roberta_qna_model = RobertaForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\", cache_dir=\".cache/huggingface/transformers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9c34ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robertaqna(question, text):\n",
    "    inputs = roberta_qna_tokenizer(question, text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = roberta_qna_model(**inputs)\n",
    "\n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    answer = roberta_qna_tokenizer.decode(predict_answer_tokens)\n",
    "            \n",
    "    if answer == \"<s>\":\n",
    "        answer = \"javob topilmadi\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350549f4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## SAYFER AI ni ishga tushirish funksiyasi (Voice activation = 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c2bd9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_no_voice_activation():\n",
    "    query = drop_characters(speech_recognizer.recognize_once_async().get().text.lower())\n",
    "    answer = day_filter(year_filter(get_response(query)))\n",
    "    print(query)\n",
    "\n",
    "    #Wikipedia knowledge base inclusion (Wikipedia dan qidirish)\n",
    "    if \"haqida\" in query and robot_name in query:\n",
    "        try:\n",
    "            \n",
    "            wiki_question = drop_word(query, robot_name).split(' haqida', 1)[0]\n",
    "            suggested_wiki_answer = wikipedia.suggest(f\"{wiki_question}\")             \n",
    "\n",
    "            # if (suggested_wiki_answer is not None):\n",
    "            #     wiki_answer = random.choice(suggested_wiki_answer.options)\n",
    "            try:\n",
    "                speech_synthesizer.speak_text_async(f\"{wiki_question} haqida qidiryabman\").get()\n",
    "                wiki_answer = wikipedia.summary(wiki_question, sentences=3)\n",
    "            except wikipedia.DisambiguationError as e:\n",
    "                s = e.options[-1]\n",
    "                wiki_answer = wikipedia.summary(s, sentences=3)\n",
    "\n",
    "            speech_synthesizer.speak_text_async(day_filter(year_filter(wiki_answer))).get()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    #Dasturlarni ochish\n",
    "    if(\"Xrom\" in answer and robot_name in query):\n",
    "        speech_synthesizer.speak_text_async(answer)\n",
    "        Popen(\"c:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\")\n",
    "        \n",
    " \n",
    "    elif(\"Kalkulyator\" in answer and robot_name in query):\n",
    "        speech_synthesizer.speak_text_async(answer)\n",
    "        call([\"calc.exe\"])\n",
    "        \n",
    "\n",
    "    elif(\"Telegram\" in answer and robot_name in query):\n",
    "        speech_synthesizer.speak_text_async(answer)\n",
    "        subprocess.Popen(\"c:\\\\Users\\\\Peter\\\\AppData\\\\Roaming\\\\Telegram Desktop\\\\Telegram.exe\")\n",
    "\n",
    "    #Soatni so'rash\n",
    "    if \"soat\" in query and robot_name in query.split():\n",
    "        currentTime = get_current_time()\n",
    "        speech_synthesizer.speak_text_async(currentTime).get()\n",
    "    \n",
    "    #TakeNotes malumotlarni saqlab qolish\n",
    "\n",
    "#     if \"eslab qol\" in query and \"sayfer\" in query.split():\n",
    "#         note = query.split(\"eslab qol\")[0]\n",
    "#         print(note)\n",
    "\n",
    "\n",
    "#         k = open('knowledgeNotes.json', 'w')\n",
    "#         k.write(json.dumps(knowledgeNotes))\n",
    "#         k.close()\n",
    "\n",
    "#         g = open('generalNotes.json', 'w')\n",
    "#         g.write(json.dumps(generalNotes))\n",
    "#         g.close()\n",
    "#         return 1\n",
    "\n",
    "\n",
    "#     #ReadNotes Ma'lumotlarni o'qish\n",
    "#     if answer == \"Bugungi kiritilgan ilmiy ma'lumotlarni o'qib eshittiraman\":\n",
    "#         if(os.path.isfile('knowledgeNotes.json')):\n",
    "#             knowledgeNotesRaw = open('knowledgeNotes.json')\n",
    "#             knowledgeNotes = json.load(knowledgeNotesRaw)\n",
    "#             knowledgeNotesRaw.close()\n",
    "#         else:\n",
    "#             speech_synthesizer.speak_text_async(\"Ilmiy ma'lumotlar bazasi topilmadi\")\n",
    "\n",
    "#         knowledgeNotesText = []\n",
    "#         knowledgeNotesDate = []\n",
    "\n",
    "\n",
    "#         for q in knowledgeNotes[\"dates\"]:\n",
    "#             date = datetime.strptime(q, '%y-%m-%d')\n",
    "#             date_final = date.strftime('%y-%m-%d')\n",
    "#             knowledgeNotesDate.append(date_final)\n",
    "\n",
    "#         for i in knowledgeNotes[\"notes\"]:\n",
    "#             knowledgeNotesText.append(i)\n",
    "\n",
    "#         knowledgeNotesArray = np.stack((knowledgeNotesText, knowledgeNotesDate), axis=1)\n",
    "\n",
    "#         speakingKnowledgeNotes = []\n",
    "\n",
    "#         for data in knowledgeNotesArray:\n",
    "#             if data[1] == mydate:\n",
    "#                 speakingKnowledgeNotes.append(data[0])\n",
    "\n",
    "\n",
    "#         if len(speakingKnowledgeNotes) == 0:\n",
    "#             speech_synthesizer.speak_text_async(\"Bugungi kiritilgan ilmiy ma'lumotlar mavjud emas\")\n",
    "#         else:\n",
    "#             speech_synthesizer.speak_text_async(\"Bugungi kiritilgan ilmiy ma'lumotlarni o'qib eshittiraman\")\n",
    "#             for note in speakingKnowledgeNotes:\n",
    "#                 speech_synthesizer.speak_text_async(note)\n",
    "#         return 1\n",
    "\n",
    "#         for k in generalNotes[\"dates\"]:\n",
    "#             date = datetime.strptime(k, '%y-%m-%d')\n",
    "#             date_final = date.strftime('%y-%m-%d')\n",
    "#             generalNotesDate.append(date_final)\n",
    "\n",
    "#         for t in generalNotes[\"notes\"]:\n",
    "#             generalNotesText.append(t)\n",
    "\n",
    "#         generalNotesArray = np.stack((generalNotesText, generalNotesDate), axis=1)\n",
    "\n",
    "#         speakingGeneralNotes = []\n",
    "\n",
    "#         for data in generalNotesArray:\n",
    "#             if data[1] == mydate:\n",
    "#                 speakingGeneralNotes.append(data[0])\n",
    "\n",
    "#         if len(speakingGeneralNotes) == 0:\n",
    "#             speech_synthesizer.speak_text_async(\"Bugungi kiritilgan umumiy ma'lumotlar mavjud emas\")\n",
    "#         else:\n",
    "#             speech_synthesizer.speak_text_async(\"Bugungi kiritilgan umumiy ma'lumotlarni o'qib eshittiraman\")\n",
    "#             for note in speakingGeneralNotes:\n",
    "#                 speech_synthesizer.speak_text_async(note)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72ad6e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 11) SAYFER AI ni ishga tushirish funksiyasi (Voice activation = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c41ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_voice_activation():\n",
    "    sayfer_status = \"offline\" \n",
    "\n",
    "    if(sayfer_status == \"offline\"):\n",
    "        query = speech_recognizer.recognize_once_async().get().text\n",
    "        print(query)\n",
    "        if(robot_name in query.lower()):\n",
    "            sayfer_status = \"online\"\n",
    "\n",
    "    if(sayfer_status == \"online\"):\n",
    "\n",
    "        playsound('assets\\\\audio.mp3', block=False)\n",
    "        query = speech_recognizer.recognize_once_async().get().text\n",
    "        answer = day_filter(year_filter(get_response(query)))\n",
    "        print(query)\n",
    "\n",
    "        get_response(query)\n",
    "\n",
    "        #Wikipedia knowledge base inclusion (Wikipedia dan qidirish)\n",
    "        if (\"haqida\" in query):\n",
    "            sayfer_status = \"offline\"\n",
    "            try:\n",
    "                wiki_question = query.split(' haqida', 1)[0]\n",
    "                suggested_wiki_answer = wikipedia.suggest(f\"{wiki_question}\")             \n",
    "\n",
    "                try:\n",
    "                    speech_synthesizer.speak_text_async(f\"{wiki_question} haqida qidiryabman\").get()\n",
    "                    wiki_answer = wikipedia.summary(wiki_question, sentences=3)\n",
    "                except wikipedia.DisambiguationError as e:\n",
    "                    s = e.options[-1]\n",
    "                    wiki_answer = wikipedia.summary(s, sentences=3)\n",
    "\n",
    "                speech_synthesizer.speak_text_async(day_filter(year_filter(wiki_answer))).get()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        #Dasturlarni ochish\n",
    "        if(\"Xrom\" in answer):\n",
    "            call([\"chrome.exe\"])\n",
    "            sayfer_status = \"offline\"\n",
    "\n",
    "        elif(\"Kalkulyator\" in answer):\n",
    "            call([\"calc.exe\"])\n",
    "            sayfer_status = \"offline\"\n",
    "\n",
    "        elif(\"Telegram\" in answer):\n",
    "            subprocess.Popen(\"c:\\\\Users\\\\Peter\\\\AppData\\\\Roaming\\\\Telegram Desktop\\\\Telegram.exe\")\n",
    "            sayfer_status = \"offline\"\n",
    "\n",
    "\n",
    "        #Soatni so'rash\n",
    "        sayfer_status = \"online\"\n",
    "        if (\"soat\" in query.lower()):\n",
    "            currentTime = get_current_time()\n",
    "            speech_synthesizer.speak_text_async(currentTime).get()\n",
    "            sayfer_status = \"offline\"\n",
    "\n",
    "\n",
    "        if(sayfer_status != \"offline\"):\n",
    "            #Buyruq aniqlanmagan holatda\n",
    "            if(answer == \"tushunmadim\"):\n",
    "                sayfer_status = \"offline\"\n",
    "                speech_synthesizer.speak_text_async(\"tushunmadim\").get()\n",
    "            else:\n",
    "                answer = day_filter(year_filter(answer))\n",
    "                speech_synthesizer.speak_text_async(answer).get()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed4d352",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## GPT-3 ni ishga tushirish funksiyasi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "813e31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gpt3(question, chat_log=None):\n",
    "  global session_prompt\n",
    "  if chat_log is None: \n",
    "    chat_log = session_prompt \n",
    "\n",
    "  prompt_text = f'{chat_log}{restart_sequence}: {question}{start_sequence}:'\n",
    "  \n",
    "  response = openai.Completion.create(\n",
    "    engine=\"text-davinci-002\",\n",
    "    prompt=prompt_text,\n",
    "    temperature=0.9,\n",
    "    max_tokens=150,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0.6,\n",
    "    stop=[\" Human:\", \" AI:\"]\n",
    "    #stop=[\"\\n\"],\n",
    "  )\n",
    "  story = response['choices'][0].text\n",
    "\n",
    "  session_prompt = f'{chat_log}{restart_sequence}: {question}{start_sequence}: {story}:'\n",
    "  return str(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05404d62",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Umumiy dasturni ishga tushirish\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7b85617f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sayfer soatni aytib yubor \n",
      "sayfer soat \n",
      "soatda aytib yubor \n",
      "i \n",
      "sayfer soat necha bo'ldi \n",
      "sayfer kalkulyatorni ochib berdi \n",
      "\n",
      "sayfer telegraf doch ber \n",
      "i sayfer telegram'ni ochib berdi \n",
      "\n",
      "\n",
      "i \n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in function SpeechRecognitionResultPtrFuture_get> returned a result with an exception set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Github\\sayfer1.0\\Sayfer\\.venv\\lib\\site-packages\\azure\\cognitiveservices\\speech\\speech_py_impl.py:7465\u001b[0m, in \u001b[0;36mSpeechRecognitionResult.<lambda>\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   7463\u001b[0m     __swig_setmethods__\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mgetattr\u001b[39m(_s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__swig_setmethods__\u001b[39m\u001b[38;5;124m'\u001b[39m, {}))\n\u001b[1;32m-> 7465\u001b[0m \u001b[38;5;21m__setattr__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, name, value: _swig_setattr(\u001b[38;5;28mself\u001b[39m, SpeechRecognitionResult, name, value)\n\u001b[0;32m   7467\u001b[0m __swig_getmethods__ \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [71]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m voice_activation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalse\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 4\u001b[0m         \u001b[43mrun_with_no_voice_activation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m voice_activation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      6\u001b[0m         run_with_voice_activation()\n",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36mrun_with_no_voice_activation\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_no_voice_activation\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     query \u001b[38;5;241m=\u001b[39m drop_characters(\u001b[43mspeech_recognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize_once_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m      3\u001b[0m     answer \u001b[38;5;241m=\u001b[39m day_filter(year_filter(get_response(query)))\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(query)\n",
      "File \u001b[1;32mD:\\Github\\sayfer1.0\\Sayfer\\.venv\\lib\\site-packages\\azure\\cognitiveservices\\speech\\speech.py:506\u001b[0m, in \u001b[0;36mResultFuture.get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;124;03m    Waits until the result is available, and returns it.\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped_type(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\Github\\sayfer1.0\\Sayfer\\.venv\\lib\\site-packages\\azure\\cognitiveservices\\speech\\speech_py_impl.py:3645\u001b[0m, in \u001b[0;36mSpeechRecognitionResultPtrFuture.get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m)  :\n\u001b[1;32m-> 3645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_speech_py_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSpeechRecognitionResultPtrFuture_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mSystemError\u001b[0m: <built-in function SpeechRecognitionResultPtrFuture_get> returned a result with an exception set"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        if voice_activation == \"False\":\n",
    "            run_with_no_voice_activation()\n",
    "        elif voice_activation == \"True\":\n",
    "            run_with_voice_activation()\n",
    "        \n",
    "        if gpt3 == \"True\":\n",
    "            activation = speech_recognizer.recognize_once_async().get().text.lower()\n",
    "            if(robot_name in activation):\n",
    "                playsound('assets/audio.mp3', block=False)\n",
    "                qinUz = speech_recognizer.recognize_once_async().get().text.lower()\n",
    "                qinEn = uzbek_to_english(qinUz)\n",
    "                aninEn = run_gpt3(qinEn)\n",
    "                aninUz = english_to_uzbek(aninEn)\n",
    "                print(qinUz)\n",
    "                print(aninUz)\n",
    "                speech_synthesizer.speak_text_async(aninUz).get()\n",
    "                \n",
    "        if robertaqna_settings == \"True\":\n",
    "            activation = speech_recognizer.recognize_once_async().get().text.lower()\n",
    "            if(robot_name in activation):\n",
    "                playsound('assets/audio.mp3', block=False)\n",
    "                qinUz = speech_recognizer.recognize_once_async().get().text.lower()\n",
    "                qinEn = uzbek_to_english(qinUz)\n",
    "                aninEn = robertaqna(qinEn, technology_news)\n",
    "                aninUz = english_to_uzbek(aninEn)\n",
    "                print(qinUz)\n",
    "                print(aninUz)\n",
    "                speech_synthesizer.speak_text_async(year_filter(aninUz)).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cf8409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
