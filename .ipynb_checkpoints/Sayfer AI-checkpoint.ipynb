{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4dd0528",
   "metadata": {},
   "source": [
    "# Sayfer AI dasturiga xush kelibsiz! \n",
    "\n",
    "- Jarvis dasturiga qiziqadigan odamlar anchagina, yoshlikdan shu sohada biror yangilik yoki dastur chiqmasin, barchasini sinab ko'rish, foydalanishga xohishim baland bo'lgani sabab, O'zbek tilida ham shunday imkoniyatni, shunday dasturiy yengillikni yaratishga maqsad qilganman. \n",
    "\n",
    "- Albatta ishlatilgan kodlarni barchasi maniki dimeman, ba'zilari official docs dan olingan, ba'zilari stackoverflow dan olingan, va ba'zilarini esa pythonni o'rganib, o'zim 0 dan tuzib chiqganman, maqsad to'liq O'zbek tilidan ishlaydigan virtual yordamchi yaratish ekan, ortiqcha so'zlarsiz proyektdan foyda ko'rganlar imkon bo'lsa duoda eslab qo'yinglar:). -- -- Hammaga rahmat!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f642110c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 0) Initialization (kerakli librarylarni yuklash)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90d2d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import torch, string, random, json, subprocess, wikipedia, os, requests, threading, uuid, nltk, openai\n",
    "from subprocess import call\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from playsound import playsound\n",
    "from tkinter import *\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "from transformers import RobertaTokenizer, RobertaForQuestionAnswering\n",
    "\n",
    "\n",
    "mydate = datetime.today().strftime(\"%y-%m-%d\")\n",
    "\n",
    "key_file = open('../key')\n",
    "\n",
    "key_data = json.load(key_file)\n",
    "resource_key = key_data['resource_key']\n",
    "region = key_data['region']\n",
    "endpoint = key_data['endpoint']\n",
    "path = key_data['path']\n",
    "KEY_FILE = key_data['key_file']\n",
    "LOCATION = key_data['location']\n",
    "openai.api_key = key_data['openapi_key']\n",
    "\n",
    "key_file.close()\n",
    "\n",
    "\n",
    "start_sequence = \"\\nAI:\"\n",
    "restart_sequence = \"\\nHuman: \"\n",
    "\n",
    "session_prompt = \"The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\\n\\nHuman: Hello, who are you?\\nAI: I am an AI created by OpenAI. How can I help you today?\\nHuman: \"\n",
    "\n",
    "speech_key, service_region = KEY_FILE, LOCATION\n",
    "speech_config_stt = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "speech_config_stt.speech_recognition_language=\"uz-UZ\"\n",
    "#audio_config_stt = speechsdk.audio.AudioConfig(device_name=\"{0.0.1.00000000}.{c91342cf-e781-4464-93ef-0a8e17804c64}\") #headset\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config_stt)\n",
    "\n",
    "audio_config_tts = speechsdk.audio.AudioConfig(device_name='c91342cf-e781-4464-93ef-0a8e17804c64') #computer\n",
    "speech_config_tts = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "speech_config_tts.speech_synthesis_voice_name = \"uz-UZ-MadinaNeural\"\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config_tts)\n",
    "\n",
    "conf_file = open('assets/settings.conf')\n",
    "conf_data = json.load(conf_file)\n",
    "\n",
    "voice_activation = conf_data['voice_activation']\n",
    "robot_name = conf_data['robot_name']\n",
    "\n",
    "conf_file.close()\n",
    "\n",
    "generalNotes = {\n",
    "      \"notes\" : [],\n",
    "      \"dates\" : []\n",
    "}\n",
    "\n",
    "knowledgeNotes = {\n",
    "      \"notes\" : [],\n",
    "      \"dates\" : []\n",
    "}\n",
    "\n",
    "#Loading existing data\n",
    "if os.path.isfile('assets/knowledgeNotes.json'):\n",
    "    knowledgeNotesRaw = open('assets/knowledgeNotes.json')\n",
    "    knowledgeNotes = json.load(knowledgeNotesRaw)\n",
    "    knowledgeNotesRaw.close()\n",
    "\n",
    "if os.path.isfile('assets/generalNotes.json'):\n",
    "    generalNotesRaw = open('assets/generalNotes.json')\n",
    "    generalNotes = json.load(generalNotesRaw)\n",
    "    generalNotesRaw.close()\n",
    "    \n",
    "wikipedia.set_lang(\"uz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e58c8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 1) Birinchi NLTK library dan foydalanib ishlatiladigan funksyilarni ishga tushiramiz.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "465061e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(sentence):\n",
    "\treturn nltk.word_tokenize(sentence)\n",
    "\n",
    "def stem(word):\n",
    "\treturn stemmer.stem(word.lower())\n",
    "\n",
    "\n",
    "def bag_of_words(tokenized_sentence, all_words):\n",
    "\ttokenized_sentence = [stem(w) for w in tokenized_sentence]\n",
    "\n",
    "\tbag = np.zeros(len(all_words), dtype=np.float32)\n",
    "\tfor idx, w in enumerate(all_words):\n",
    "\t\tif w in tokenized_sentence:\n",
    "\t\t\tbag[idx] = 1.0\n",
    "\n",
    "\treturn bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad0ddd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 2) NLTK Utils to'liq ishlashi uchun punkt modeli kerak bo'ladi, endi uni yuklab olishimiz kerak.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f48601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Peter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83803aad",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 3) Uchinchi qilgan ishimiz, pytorch orqali modelni yaratish, va uni yuklash\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "702b3432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\tdef __init__(self, input_size, hidden_size, num_classes):\n",
    "\t\tsuper(NeuralNet, self).__init__()\n",
    "\t\tself.l1 = nn.Linear(input_size, hidden_size)\n",
    "\t\tself.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "\t\tself.l3 = nn.Linear(hidden_size, num_classes)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = self.l1(x)\n",
    "\t\tout = self.relu(out)\n",
    "\t\tout = self.l2(out)\n",
    "\t\tout = self.relu(out)\n",
    "\t\tout = self.l3(out)\n",
    "\n",
    "\t\t#no activation no softmax\n",
    "\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0e96a8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 4) Endi novbat, modelni intents.json fayl imiz orqali shug'ullantirish (training)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69789d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.4873\n",
      "Epoch [200/1000], Loss: 0.0384\n",
      "Epoch [300/1000], Loss: 0.0060\n",
      "Epoch [400/1000], Loss: 0.0032\n",
      "Epoch [500/1000], Loss: 0.0016\n",
      "Epoch [600/1000], Loss: 0.0008\n",
      "Epoch [700/1000], Loss: 0.0002\n",
      "Epoch [800/1000], Loss: 0.0001\n",
      "Epoch [900/1000], Loss: 0.0001\n",
      "Epoch [1000/1000], Loss: 0.0001\n",
      "final loss: 0.0001\n",
      "training complete. file saved to model/data-07-07-2022-01-13-55.pth\n"
     ]
    }
   ],
   "source": [
    "with open('assets/intents.json') as f:\n",
    "\tintents = json.load(f)\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "\n",
    "for intent in intents['intents']:\n",
    "\ttag = intent['tag']\n",
    "\ttags.append(tag)\n",
    "\tfor pattern in intent['patterns']:\n",
    "\t\tw = tokenize(pattern)\n",
    "\t\tall_words.extend(w)\n",
    "\t\txy.append((w, tag))\n",
    "\n",
    "ignore_words = ['?', '!', ',', '.']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for (pattern_sentence, tag) in xy:\n",
    "\tbag = bag_of_words(pattern_sentence, all_words)\n",
    "\tx_train.append(bag)\n",
    "\n",
    "\tlabel = tags.index(tag)\n",
    "\ty_train.append(label)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "\tdef __init__(self):\n",
    "\t\tself.n_samples = len(x_train)\n",
    "\t\tself.x_data = x_train\n",
    "\t\tself.y_data = y_train\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn self.x_data[index], self.y_data[index]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.n_samples\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "input_size = len(x_train[0])\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "\n",
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(words)\n",
    "        # if y would be one-hot, we must apply\n",
    "        # labels = torch.max(labels, 1)[1]\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "print(f'final loss: {loss.item():.4f}')\n",
    "\n",
    "data = {\n",
    "\"model_state\": model.state_dict(),\n",
    "\"input_size\": input_size,\n",
    "\"hidden_size\": hidden_size,\n",
    "\"output_size\": output_size,\n",
    "\"all_words\": all_words,\n",
    "\"tags\": tags\n",
    "}\n",
    "\n",
    "date = datetime.now().strftime('%d-%m-%Y-%H-%M-%S')\n",
    "\n",
    "FILE = f\"model/data-{date}.pth\"\n",
    "\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802d400",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 5) Modeldan javob olish\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed36ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open('assets/intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "FILE = \"model/data-noloss.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data[\"all_words\"]\n",
    "tags = data[\"tags\"]\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "bot_name = \"Sayfer\"\n",
    "\n",
    "def get_response(msg):\n",
    "\n",
    "    sentence = tokenize(msg)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "\n",
    "    if prob.item() > 0.75:\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                answer = random.choice(intent['responses'])\n",
    "                return answer\n",
    "    \n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9f493",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 6) Savol va Javoblarni Ingliz tilidan Uzbek tiliga va Uzbek tilidan Ingliz tiliga tarjima qilish funksiyalari\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6666df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_to_uzbek(text):\n",
    "    params = '&from=en&to=uz'\n",
    "    constructed_url = endpoint + path + params\n",
    "\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': resource_key,\n",
    "        'Ocp-Apim-Subscription-Region': region,\n",
    "        'Content-type': 'application/json',\n",
    "        'X-ClientTraceId': str(uuid.uuid4())\n",
    "    }\n",
    "\n",
    "    body = [{\n",
    "        'text' : str(text)\n",
    "    }]\n",
    "    request = requests.post(constructed_url, headers=headers, json=body)\n",
    "    response = request.json()\n",
    "    ans = json.loads(json.dumps(response, sort_keys=True, indent=4, separators=(',', ': ')))\n",
    "\n",
    "\n",
    "    return ans[0]['translations'][0]['text']\n",
    "\n",
    "def uzbek_to_english(text):\n",
    "    params = '&from=uz&to=en'\n",
    "    constructed_url = endpoint + path + params\n",
    "\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': resource_key,\n",
    "        'Ocp-Apim-Subscription-Region': region,\n",
    "        'Content-type': 'application/json',\n",
    "        'X-ClientTraceId': str(uuid.uuid4())\n",
    "    }\n",
    "\n",
    "    body = [{\n",
    "        'text' : str(text)\n",
    "    }]\n",
    "    request = requests.post(constructed_url, headers=headers, json=body)\n",
    "    response = request.json()\n",
    "    ans = json.loads(json.dumps(response, sort_keys=True, indent=4, separators=(',', ': ')))\n",
    "\n",
    "\n",
    "    return ans[0]['translations'][0]['text'] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffa1a0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 7) Ortiqcha belgilarni textdan olib tashlash\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c61f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_characters(text):\n",
    "    a = text\n",
    "    b = \"!@#$()-_=+%&.,\\\" \"\n",
    "    \n",
    "    for char in b:\n",
    "        a = a.replace(char, \" \")    \n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c022e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 8) Kunlar va yillar qatnashgan tekstlarni ovozli eshittirishga moslashtirish\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06805a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_first = {\n",
    "    1 : \"O'n\",\n",
    "    2 : \"Yigirma\",\n",
    "    3 : \"O'ttiz\"\n",
    "}\n",
    "\n",
    "day_second = {\n",
    "    1 : \"Birinchi\",\n",
    "    2 : \"Ikkinchi\",\n",
    "    3 : \"Uchinchi\",\n",
    "    4 : \"To'rtinchi\",\n",
    "    5 : \"Beshinchi\",\n",
    "    6 : \"Oltinchi\",\n",
    "    7 : \"Yettinchi\",\n",
    "    8 : \"Sakkizinchi\",\n",
    "    9 : \"To'qqizinchi\",\n",
    "    0 : \"inchi\"\n",
    "}\n",
    "\n",
    "\n",
    "def day_filter(answer):\n",
    "    answer = drop_characters(answer)\n",
    "\n",
    "    digits = [int(s) for s in answer.split() if s.isdigit() and int(s) <= 31 and int(s) >= 10]\n",
    "    digits2 = [int(s) for s in answer.split() if s.isdigit() and int(s) <= 9]\n",
    "\n",
    "    digits_in_text = []\n",
    "    digits_in_text2 = []\n",
    "\n",
    "    for digit in digits:\n",
    "        sp = [int(a) for a in str(digit)]\n",
    "        digits_in_text.append(sp)\n",
    "\n",
    "    for digit in digits2:\n",
    "        sp = [int(a) for a in str(digit)]\n",
    "        digits_in_text2.append(sp)\n",
    "\n",
    "    day_first_values = []\n",
    "    day_second_values = []\n",
    "\n",
    "    day_first_values2 = []\n",
    "\n",
    "    for p in digits_in_text:\n",
    "        day_first_values.append(str(p[0]).replace(str(p[0]), day_first[p[0]]))\n",
    "        day_second_values.append(str(p[1]).replace(str(p[1]), day_second[p[1]]))\n",
    "\n",
    "\n",
    "    for p in digits_in_text2:\n",
    "        day_first_values2.append(str(p[0]).replace(str(p[0]), day_second[p[0]]))\n",
    "\n",
    "    joint_days = {}\n",
    "    joint_days2 = {}\n",
    "\n",
    "    for t in range(0, len(digits)):\n",
    "        days_in_text = day_first_values[t] + \" \" + day_second_values[t]\n",
    "        joint_days[digits[t]] = days_in_text \n",
    "\n",
    "    for t in range(0, len(digits2)):\n",
    "        days_in_text2 = day_first_values2[t]\n",
    "        joint_days2[digits2[t]] = days_in_text2 \n",
    "\n",
    "    for _ in range(0, len(digits)):\n",
    "        for t in answer.split():\n",
    "            if(t.isdigit() and int(t) <= 31 and int(t) >= 10):\n",
    "                answer = answer.replace(t, joint_days[int(t)])\n",
    "\n",
    "    for _ in range(0, len(digits2)):\n",
    "        for t in answer.split():\n",
    "            if(t.isdigit() and int(t) <= 9):\n",
    "                answer = answer.replace(t, joint_days2[int(t)])\n",
    "\n",
    "    return answer\n",
    "\n",
    "def year_filter(answer):\n",
    "    year_first = {\n",
    "        1 : \"bir ming\",\n",
    "        2 : \"ikki ming\",\n",
    "        3 : \"uch ming\",\n",
    "        4 : \"to'rt ming\",\n",
    "        5 : \"besh ming\",\n",
    "        6 : \"olti ming\",\n",
    "        7 : \"yetti ming\",\n",
    "        8 : \"sakkiz ming\",\n",
    "        9 : \"to'qqiz ming\"\n",
    "    }\n",
    "\n",
    "    year_second = {\n",
    "        1 : \"bir yuz\",\n",
    "        2 : \"ikkiyuz\",\n",
    "        3 : \"uchyuz\",\n",
    "        4 : \"to'rtyuz\",\n",
    "        5 : \"beshyuz\",\n",
    "        6 : \"oltiyuz\",\n",
    "        7 : \"yettiyuz\",\n",
    "        8 : \"sakkizyuz\",\n",
    "        9 : \"to'qqizyuz\",\n",
    "        0 : \"\"\n",
    "    }\n",
    "\n",
    "    year_third = {\n",
    "        1 : \"o'n\",\n",
    "        2 : \"yigirma\",\n",
    "        3 : \"o'ttiz\",\n",
    "        4 : \"qirq\",\n",
    "        5 : \"ellik\",\n",
    "        6 : \"oltmish\",\n",
    "        7 : \"yetmish\",\n",
    "        8 : \"sakson\",\n",
    "        9 : \"to'qson\",\n",
    "        0 : \"\"\n",
    "\n",
    "    }\n",
    "\n",
    "    year_fourth = {\n",
    "        1 : \"birinchi\",\n",
    "        2 : \"ikkinchi\",\n",
    "        3 : \"uchinchi\",\n",
    "        4 : \"to'rtinchi\",\n",
    "        5 : \"beshinchi\",\n",
    "        6 : \"oltinchi\",\n",
    "        7 : \"yettinchi\",\n",
    "        8 : \"sakkizinchi\",\n",
    "        9 : \"to'qqizzinchi\",\n",
    "        0 : \"inchi\"\n",
    "    }\n",
    "\n",
    "    answer = drop_characters(answer)\n",
    "\n",
    "    digits = [int(s) for s in answer.split() if s.isdigit() and int(s) >= 1000]\n",
    "    \n",
    "    digits_in_text = []\n",
    "    for k in digits:\n",
    "        seperate_year_digits = [int(b) for b in str(k)]\n",
    "        digits_in_text.append(seperate_year_digits)\n",
    "\n",
    "    year_first_values = []\n",
    "    year_second_values = []\n",
    "    year_third_values = []\n",
    "    year_fourth_values = []\n",
    "\n",
    "\n",
    "    for p in digits_in_text:\n",
    "        year_first_values.append(str(p[0]).replace(str(p[0]), year_first[p[0]]))\n",
    "        year_second_values.append(str(p[1]).replace(str(p[1]), year_second[p[1]]))\n",
    "        year_third_values.append(str(p[2]).replace(str(p[2]), year_third[p[2]]))\n",
    "        year_fourth_values.append(str(p[3]).replace(str(p[3]), year_fourth[p[3]]))\n",
    "\n",
    "    joint_years = {}\n",
    "    for t in range(0, len(digits)):\n",
    "        years_in_text = year_first_values[t] + \" \" + year_second_values[t] + \" \" + year_third_values[t] + \" \" + year_fourth_values[t]\n",
    "        joint_years[digits[t]] = years_in_text \n",
    "\n",
    "\n",
    "    for _ in range(0, len(digits)):\n",
    "        for t in answer.split():\n",
    "            if(t.isdigit() and int(t) >= 1000):\n",
    "                answer = answer.replace(t, joint_years[int(t)])\n",
    "\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd8cb4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 9) Ayni vaqtni aniqlash funksiyasi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "238768b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_time():\n",
    "    answer = datetime.now().strftime(\"%H-%M\").split(\"-\",1)\n",
    "    hour = int(answer[0])\n",
    "    minute = int(answer[1])\n",
    "\n",
    "    if (minute > 9):\n",
    "        minute_digits = [int(a) for a in str(minute)]\n",
    "        minute_last = minute_digits[1]\n",
    "        minute_first = minute_digits[0]\n",
    "\n",
    "    minute_say = \"\"\n",
    "\n",
    "    if minute in range(10, 20):\n",
    "        minute_say = \"o'n\"\n",
    "    elif minute in range(20, 30):\n",
    "        minute_say = \"yigirma\"\n",
    "    elif minute in range(30, 40):\n",
    "        minute_say = \"o'ttiz\"\n",
    "    elif minute in range(40, 50):\n",
    "        minute_say = \"qirq\"\n",
    "    elif minute in range(50, 60):\n",
    "        minute_say = \"ellik\"\n",
    "\n",
    "\n",
    "\n",
    "    if (hour > 9):\n",
    "        hour_digits = [int(a) for a in str(hour)]\n",
    "        hour_last = hour_digits[1]\n",
    "        hour_first = hour_digits[0]\n",
    "\n",
    "    hour_say = \"\"\n",
    "\n",
    "    if hour in range(10, 20):\n",
    "        hour_say = \"o'n\"\n",
    "    elif hour in range(20, 24):\n",
    "        hour_say = \"yigirma\"\n",
    "\n",
    "\n",
    "\n",
    "    if(minute <= 9 and hour <= 9):\n",
    "        currentTime = f\"Soat {hour} dan, {minute} daqiqa o'tdi.\"\n",
    "    elif(minute >= 10 and hour <= 9):\n",
    "        currentTime = f\"Soat {hour} dan, {minute_say} {minute_last} daqiqa o'tdi.\"\n",
    "    elif(minute <= 9 and hour >= 10):\n",
    "        currentTime = f\"Soat {hour_say} {hour_last} dan, {minute} daqiqa o'tdi.\"\n",
    "    elif(minute >= 10 and hour >= 10 and hour_last != 0):\n",
    "        currentTime = f\"Soat {hour_say} {hour_last} dan, {minute_say} {minute_last} daqiqa o'tdi.\"\n",
    "    elif(minute >= 10 and hour >= 10 and hour_last == 0):\n",
    "        currentTime = f\"Soat {hour_say} dan, {minute} daqiqa o'tdi.\"\n",
    "        \n",
    "    return currentTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de377056",
   "metadata": {},
   "source": [
    "## 10) Tekst klassifikatsiyasi modelini yuklash (text classification model from huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a505026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", cache_dir=\".cache/huggingface/transformers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e90931ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_classifier(query):\n",
    "    labels = [\"artifacts\", \"animals\", \"food\", \"birds\", \"fast food\", \"book\", \"education\", \"news\", \"failure\", \"emotion\", \"sad\",\n",
    "             \"happy\", \"time\", \"family\", \"math\", \"computer\", \"general\", \"language\", \"drinks\", \"alcohol\", \"religion\", \"Islam\",\n",
    "             \"cars\"]\n",
    "    hypothesis_template = \"This text is about {} .\"\n",
    "    prediction = text_classifier(query, labels, hypothesis_template=hypothesis_template, multi_label=True)\n",
    "\n",
    "    labels = prediction['labels'][:3]\n",
    "    scores = prediction['scores'][:3]\n",
    "    labels_selected = []\n",
    "    scores_selected = []\n",
    "    final_answer = []\n",
    "    #Qaytarilgan javoblardan 3 ta 75 dan balandlarini tanlab olish\n",
    "    for score in scores:\n",
    "        if score >= 0.75:\n",
    "            scores_selected.append(score)\n",
    "# Eng yaxshi natijani aniqlash\n",
    "    if len(scores_selected) == 1:\n",
    "        labels_selected.append(labels[0])\n",
    "    elif len(scores_selected) == 2:\n",
    "        labels_selected.append(labels[0])\n",
    "        labels_selected.append(labels[1])\n",
    "    elif len(scores_selected) == 3:\n",
    "        labels_selected.append(labels[0])\n",
    "        labels_selected.append(labels[1])\n",
    "        labels_selected.append(labels[2])\n",
    "    \n",
    "    if len(scores_selected) != 0:\n",
    "        final_answer.append(scores_selected[0])\n",
    "        final_answer.append(labels_selected[0])\n",
    "    else:\n",
    "        final_answer = \"No response\"\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b6425",
   "metadata": {},
   "source": [
    "### 11) Savollarga javob berish va o'rganish modelini yuklash ( huggingface, roberta-base-squad2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bba88f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_qna_tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\", cache_dir=\".cache/huggingface/transformers/\")\n",
    "roberta_qna_model = RobertaForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\", cache_dir=\".cache/huggingface/transformers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c34ad52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' very hard'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"how was your day?\"\n",
    "text = \"it has been very hard day today, no hard feelings, just drove a car for 6 hours\"\n",
    "\n",
    "inputs = roberta_qna_tokenizer(question, text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = roberta_qna_model(**inputs)\n",
    "\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "roberta_qna_tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350549f4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## SAYFER AI ni ishga tushirish funksiyasi (Voice activation = 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_no_voice_activation():\n",
    "    query = drop_characters(speech_recognizer.recognize_once_async().get().text).lower()\n",
    "    answer = day_filter(year_filter(get_response(query)))\n",
    "    print(query)\n",
    "\n",
    "    get_response(query)\n",
    "\n",
    "    #Wikipedia knowledge base inclusion (Wikipedia dan qidirish)\n",
    "    if \"haqida\" in query and \"sayfer\" in query.split():\n",
    "        try:\n",
    "            words = query.split()\n",
    "            for word in words:\n",
    "                if word == \"sayfer\":\n",
    "                    words.remove(word)\n",
    "\n",
    "            joint_query = ' '.join(words)\n",
    "            wiki_question = joint_query.split(' haqida', 1)[0]\n",
    "            suggested_wiki_answer = wikipedia.suggest(f\"{wiki_question}\")             \n",
    "\n",
    "            # if (suggested_wiki_answer is not None):\n",
    "            #     wiki_answer = random.choice(suggested_wiki_answer.options)\n",
    "            try:\n",
    "                speech_synthesizer.speak_text_async(f\"{wiki_question} haqida qidiryabman\").get()\n",
    "                wiki_answer = wikipedia.summary(wiki_question, sentences=3)\n",
    "            except wikipedia.DisambiguationError as e:\n",
    "                s = e.options[-1]\n",
    "                wiki_answer = wikipedia.summary(s, sentences=3)\n",
    "\n",
    "            speech_synthesizer.speak_text_async(day_filter(year_filter(wiki_answer))).get()\n",
    "            return 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    #Dasturlarni ochish\n",
    "    if(\"Xrom\" in answer and \"sayfer\" in query.lower()):\n",
    "        speech_synthesizer.speak_text_async(answer)\n",
    "        call([\"chrome.exe\"])\n",
    "        return 1\n",
    "    elif(\"Kalkulyator\" in answer and \"sayfer\" in query.lower()):\n",
    "        speech_synthesizer.speak_text_async(answer)\n",
    "        call([\"calc.exe\"])\n",
    "        return 1\n",
    "    elif(\"Telegram\" in answer and \"sayfer\" in query.lower()):\n",
    "        speech_synthesizer.speak_text_async(answer)\n",
    "        subprocess.Popen(\"c:\\\\Users\\\\Peter\\\\AppData\\\\Roaming\\\\Telegram Desktop\\\\Telegram.exe\")\n",
    "        return 1\n",
    "\n",
    "    #Soatni so'rash\n",
    "    if \"soat\" in query.lower() and \"sayfer\" in query.lower().split():\n",
    "        currentTime = get_current_time()\n",
    "        speech_synthesizer.speak_text_async(currentTime).get()\n",
    "        return 1\n",
    "    #TakeNotes malumotlarni saqlab qolish\n",
    "\n",
    "    if answer == \"Ilmiy ma'lumotlarni saqlayman\" and \"sayfer\" in query.lower().split():\n",
    "        note = query.split(\"eslab qol\")[0]\n",
    "        print(note)\n",
    "\n",
    "        if(\"umumiy\" in query):\n",
    "            note = query.split(\"umumiy\")[0]\n",
    "            speech_synthesizer.speak_text_async(\"umumiy ma'lumot kiritildi\").get()\n",
    "            generalNotes[\"notes\"].append(note)\n",
    "            generalNotes[\"dates\"].append(str(mydate))\n",
    "\n",
    "        elif(\"ilmiy\" in query):\n",
    "            note = query.split(\"ilmiy\")[0]\n",
    "            speech_synthesizer.speak_text_async(\"Ilmiy ma'lumot kiritildi\").get()\n",
    "            knowledgeNotes[\"notes\"].append(note)\n",
    "            knowledgeNotes[\"dates\"].append(str(mydate))\n",
    "\n",
    "        else:\n",
    "            speech_synthesizer.speak_text_async(\"Qaysi datasetga ma'lumot kiritayotganingizni ayting: ilmiy yoki umumiy\").get()\n",
    "\n",
    "            playsound('audio.mp3', block=False)\n",
    "            confirmation = speech_recognizer.recognize_once_async().get().text\n",
    "\n",
    "            if(\"ilmiy\" in confirmation.lower()):\n",
    "                speech_synthesizer.speak_text_async(\"ilmiy ma'lumotlar bazasiga kiritaman\").get()\n",
    "                knowledgeNotes[\"notes\"].append(note)\n",
    "                knowledgeNotes[\"dates\"].append(str(mydate))\n",
    "\n",
    "            elif(\"umumiy\" in confirmation.lower()):\n",
    "                speech_synthesizer.speak_text_async(\"umumiy ma'lumotlar bazasiga kiritaman\").get()\n",
    "                generalNotes[\"notes\"].append(note)\n",
    "                generalNotes[\"dates\"].append(str(mydate))\n",
    "\n",
    "            else:          \n",
    "              speech_synthesizer.speak_text_async(\"Bunday ma'lumotlar ba'zasi topilmadi, saqlashni bekor qilaman\").get()\n",
    "\n",
    "\n",
    "        k = open('knowledgeNotes.json', 'w')\n",
    "        k.write(json.dumps(knowledgeNotes))\n",
    "        k.close()\n",
    "\n",
    "        g = open('generalNotes.json', 'w')\n",
    "        g.write(json.dumps(generalNotes))\n",
    "        g.close()\n",
    "        return 1\n",
    "\n",
    "\n",
    "    #ReadNotes Ma'lumotlarni o'qish\n",
    "    if answer == \"Bugungi kiritilgan ilmiy ma'lumotlarni o'qib eshittiraman\":\n",
    "        if(os.path.isfile('knowledgeNotes.json')):\n",
    "            knowledgeNotesRaw = open('knowledgeNotes.json')\n",
    "            knowledgeNotes = json.load(knowledgeNotesRaw)\n",
    "            knowledgeNotesRaw.close()\n",
    "        else:\n",
    "            speech_synthesizer.speak_text_async(\"Ilmiy ma'lumotlar bazasi topilmadi\")\n",
    "\n",
    "        knowledgeNotesText = []\n",
    "        knowledgeNotesDate = []\n",
    "\n",
    "\n",
    "        for q in knowledgeNotes[\"dates\"]:\n",
    "            date = datetime.strptime(q, '%y-%m-%d')\n",
    "            date_final = date.strftime('%y-%m-%d')\n",
    "            knowledgeNotesDate.append(date_final)\n",
    "\n",
    "        for i in knowledgeNotes[\"notes\"]:\n",
    "            knowledgeNotesText.append(i)\n",
    "\n",
    "        knowledgeNotesArray = np.stack((knowledgeNotesText, knowledgeNotesDate), axis=1)\n",
    "\n",
    "        speakingKnowledgeNotes = []\n",
    "\n",
    "        for data in knowledgeNotesArray:\n",
    "            if data[1] == mydate:\n",
    "                speakingKnowledgeNotes.append(data[0])\n",
    "\n",
    "\n",
    "        if len(speakingKnowledgeNotes) == 0:\n",
    "            speech_synthesizer.speak_text_async(\"Bugungi kiritilgan ilmiy ma'lumotlar mavjud emas\")\n",
    "        else:\n",
    "            speech_synthesizer.speak_text_async(\"Bugungi kiritilgan ilmiy ma'lumotlarni o'qib eshittiraman\")\n",
    "            for note in speakingKnowledgeNotes:\n",
    "                speech_synthesizer.speak_text_async(note)\n",
    "        return 1\n",
    "    if answer == \"Bugungi kiritilgan umumiy ma'lumotlarni o'qib eshittiraman\":\n",
    "\n",
    "        if(os.path.isfile('generalNotes.json')):  \n",
    "            generalNotesRaw = open('generalNotes.json')\n",
    "            generalNotes = json.load(generalNotesRaw)\n",
    "            generalNotesRaw.close()\n",
    "        else:\n",
    "            speech_synthesizer.speak_text_async(\"Umumiy ma'lumotlar bazasi topilmadi\")\n",
    "\n",
    "        generalNotesText = []\n",
    "        generalNotesDate = []\n",
    "\n",
    "\n",
    "        for k in generalNotes[\"dates\"]:\n",
    "            date = datetime.strptime(k, '%y-%m-%d')\n",
    "            date_final = date.strftime('%y-%m-%d')\n",
    "            generalNotesDate.append(date_final)\n",
    "\n",
    "        for t in generalNotes[\"notes\"]:\n",
    "            generalNotesText.append(t)\n",
    "\n",
    "        generalNotesArray = np.stack((generalNotesText, generalNotesDate), axis=1)\n",
    "\n",
    "        speakingGeneralNotes = []\n",
    "\n",
    "        for data in generalNotesArray:\n",
    "            if data[1] == mydate:\n",
    "                speakingGeneralNotes.append(data[0])\n",
    "\n",
    "        if len(speakingGeneralNotes) == 0:\n",
    "            speech_synthesizer.speak_text_async(\"Bugungi kiritilgan umumiy ma'lumotlar mavjud emas\")\n",
    "        else:\n",
    "            speech_synthesizer.speak_text_async(\"Bugungi kiritilgan umumiy ma'lumotlarni o'qib eshittiraman\")\n",
    "            for note in speakingGeneralNotes:\n",
    "                speech_synthesizer.speak_text_async(note)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72ad6e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 11) SAYFER AI ni ishga tushirish funksiyasi (Voice activation = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_voice_activation():\n",
    "    sayfer_status = \"offline\" \n",
    "\n",
    "    if(sayfer_status == \"offline\"):\n",
    "        query = speech_recognizer.recognize_once_async().get().text\n",
    "        print(query)\n",
    "        if(robot_name in query.lower()):\n",
    "            sayfer_status = \"online\"\n",
    "\n",
    "    if(sayfer_status == \"online\"):\n",
    "\n",
    "        playsound('audio.mp3', block=False)\n",
    "        query = speech_recognizer.recognize_once_async().get().text\n",
    "        answer = day_filter(year_filter(get_response(query)))\n",
    "        print(query)\n",
    "\n",
    "        get_response(query)\n",
    "\n",
    "        #Wikipedia knowledge base inclusion (Wikipedia dan qidirish)\n",
    "        if (\"haqida\" in query):\n",
    "            sayfer_status = \"offline\"\n",
    "            try:\n",
    "                wiki_question = query.split(' haqida', 1)[0]\n",
    "                suggested_wiki_answer = wikipedia.suggest(f\"{wiki_question}\")             \n",
    "\n",
    "                try:\n",
    "                    speech_synthesizer.speak_text_async(f\"{wiki_question} haqida qidiryabman\").get()\n",
    "                    wiki_answer = wikipedia.summary(wiki_question, sentences=3)\n",
    "                except wikipedia.DisambiguationError as e:\n",
    "                    s = e.options[-1]\n",
    "                    wiki_answer = wikipedia.summary(s, sentences=3)\n",
    "\n",
    "                speech_synthesizer.speak_text_async(day_filter(year_filter(wiki_answer))).get()\n",
    "\n",
    "                return 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        #Dasturlarni ochish\n",
    "        if(\"Xrom\" in answer):\n",
    "            call([\"chrome.exe\"])\n",
    "            sayfer_status = \"offline\"\n",
    "            return 1\n",
    "        elif(\"Kalkulyator\" in answer):\n",
    "            call([\"calc.exe\"])\n",
    "            sayfer_status = \"offline\"\n",
    "            return 1\n",
    "        elif(\"Telegram\" in answer):\n",
    "            subprocess.Popen(\"c:\\\\Users\\\\Peter\\\\AppData\\\\Roaming\\\\Telegram Desktop\\\\Telegram.exe\")\n",
    "            sayfer_status = \"offline\"\n",
    "            return 1\n",
    "\n",
    "        #Soatni so'rash\n",
    "        sayfer_status = \"online\"\n",
    "        if (\"soat\" in query.lower()):\n",
    "            currentTime = get_current_time()\n",
    "            speech_synthesizer.speak_text_async(currentTime).get()\n",
    "            sayfer_status = \"offline\"\n",
    "            return 1\n",
    "\n",
    "        if(sayfer_status != \"offline\"):\n",
    "            #Buyruq aniqlanmagan holatda\n",
    "            if(answer == \"tushunmadim\"):\n",
    "                sayfer_status = \"offline\"\n",
    "                speech_synthesizer.speak_text_async(\"tushunmadim\").get()\n",
    "            else:\n",
    "                answer = day_filter(year_filter(answer))\n",
    "                speech_synthesizer.speak_text_async(answer).get()\n",
    "                return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed4d352",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## GPT-3 ni ishga tushirish funksiyasi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gpt3(question, chat_log=None):\n",
    "  global session_prompt\n",
    "  if chat_log is None: \n",
    "    chat_log = session_prompt \n",
    "\n",
    "  prompt_text = f'{chat_log}{restart_sequence}: {question}{start_sequence}:'\n",
    "  \n",
    "  response = openai.Completion.create(\n",
    "    engine=\"text-davinci-002\",\n",
    "    prompt=prompt_text,\n",
    "    temperature=0.9,\n",
    "    max_tokens=150,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0.6,\n",
    "    stop=[\" Human:\", \" AI:\"]\n",
    "    #stop=[\"\\n\"],\n",
    "  )\n",
    "  story = response['choices'][0].text\n",
    "\n",
    "  session_prompt = f'{chat_log}{restart_sequence}: {question}{start_sequence}: {story}:'\n",
    "  return str(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05404d62",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Umumiy dasturni ishga tushirish\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        #run_app()\n",
    "        #if run_app() != 1:\n",
    "        activation = speech_recognizer.recognize_once_async().get().text.lower()\n",
    "        if('sayfer' in activation):\n",
    "            playsound('audio.mp3', block=False)\n",
    "            qinUz = speech_recognizer.recognize_once_async().get().text.lower()\n",
    "            qinEn = uzbek_to_english(qinUz)\n",
    "            aninEn = run_gpt3(qinEn)\n",
    "            aninUz = english_to_uzbek(aninEn)\n",
    "            print(qinUz)\n",
    "            print(aninUz)\n",
    "            speech_synthesizer.speak_text_async(aninUz).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cf8409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
